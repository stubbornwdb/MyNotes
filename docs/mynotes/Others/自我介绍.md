# 1.自我介绍
你好，很高兴有机会参加你们公司的面试。我是21年毕业的，大学学的专业就是计算机科学与技术，
毕业后入职xxx公司，主要是在xxx团队担任后端开发的角色，先后参与过多个项目设计、开发工作。
日常工作的话主要是使用 go 语言进行开发，涉及技术栈包括 mysql 、 saturn 、kafka、 
redis 等。我有较高的抗压能力，在工作中，我在多个并发高优需求下，能够很高质量地有序完成所有
需求，并善于做好个人管理；在项目不断地发展过程中，我能不断去补相关知识以解决我在工作中遇到的
问题；另外我个人主动性比较强，能够主动去了解整个团队的业务领域，了解各个业务内容以及积极跟PM
探讨需求的实现，我也经常主动去请教各个同事，讨论遇到的题、技术实现等；
以上就是我的一个基本介绍了。


你的话我建议你可以找一下：
1，抗压能力：在多个并发高优需求下，能够很高质量地有序完成所有需求，并做好个人管理
2，学习能力：在项目不断发展地过程中，你还不断地去补相关知识，以解决遇到的问题（比
如你是不是没有提，在搞python proxy时，你的header处理那部分？就返回的body一大
就导致报错的那个）
3，个人的主动性

# 2.项目介绍
## 2.1 出库模块
### 2.1.1 模块的功能
介绍：和国内所有快递业务一样，运单到达网点后，需要先进行收货，然后根据运单的数据，判断货物流向。
站内操作人员会先进行站内收货动作，收完货后，会对订单进行分拣打包。 打包时，操作人员扫描订单号后，
传给后端进行代码逻辑判断，判断是否是同一个转运类型和转运目的地。如果相同，则可以加入到同一个转运
包中。打包好后就进行交接给下游（干线运输、派送或交接给第三方物流商）。当然了，也涉及逆向流程，也
就是发起回退的流程，也同样是类似的操作，站点收货、交接。所以站内这个团队被划分为入库、出库、操作
作业单、容器、分拣机几个领域。我主要是负责打包这个模块的业务。打包模块涉及交互的领域非常多，所以
我们打包的调用链路在业务本身上就是很长的，我们核心作业接口面向的使用方主要是两个：站内操作人员以
及分拣机。

主要工作：我在这边做的最核心的一个项目就是进行了打包模块的重构。
1. 在mentor的帮助下设计并开发了一个基于DAG图的编排框架，用于将打包的各个流程进行编排，通过go
协程来异步执行各个没有依赖的流程；
2. 与产品共同厘清了打包流程中不同市场的校验逻辑，并整理出了完整的文档；
3. 重构后的代码简单易读，可维护性很高，并且增加了完整的单元测试，保证程序的可靠性；
4. 设计并开发除了基于日志的对账以及告警功能，在上线前通过对账的方式保证重构逻辑的正确性，通过一
段时间的验证后顺利完成了新代码的灰度以及切流；

代码大体逻辑是：根据调用方传入的订单号、查询运单信息、调用其他
第三方（路由、计费、收货、派送等各个域进行相关的校验）、调用容器服务将打包信息落库、查询并组装渲
染数据返回给调用方。

### 2.1.2 进行重构的原因是什么
1. 代码逻辑很混乱，可读性很差，非常难以维护。主要体现在：
- 代码中的重复的地方很多，相同功能有不同的实现，没有通用性、代码复杂度高，特别是判断订单流向的逻辑，使
用了非常多的if else嵌套；
- 滥用指针，打包的流程依赖订单这个实体数据，代码采用指针的方式进行整个链路的传递，导致这个数据中途被改
了都无法发现；
- 接口设计不合理，接口是面向SQL 编程的，入参、出参都有非常多重复或者冗余的字段，并且采用手动注入的方式注入
依赖，导致没法方便地做单测；

3. 没有单测也很难做单测

4. 因为整体链路很长，接口响应时间很长，高达700 - 800 ms,很影响作业效率。对于一些重操作没有并发机制；

5. 没有标准的日志、上报、监控规范，打的日志非常冗余并且混乱，容易干扰问题的定位、排查

以上问题导致新需求修改困难，容易触雷，且线上问题不容易排错

### 2.1.3 重构过程：
前期：
与产品梳理打包流程中各个市场的逻辑，找出各个市场的相同点、差异点；
与前端厘清接口设计的合理性，发现了大量已经废弃或者重复的字段；

开发阶段：
重新调整代码层次结构，让service 层的接口职责更加明确，原来的打包接口基本上是一个方法里面
从头到尾大包大榄，将所有逻辑都对到一块，根本没法复用，比如打包大体是分成校验 + 打包两
个阶段的，对于给站点操作人员作业用的逻辑这两个阶段是同步的，而我们提供给分拣机使用的接口，
这两个阶段是异步的，所以原来的代码将这写逻辑拆分成了三个接口。
打包领域最终其实就只需要提供校验、打包两个能力就行，所以在 service层我设计定义了两个接口，
让代码具有更高的复用性，包括后面有新需求需要实现一个自动打包的能力，也就是收货完成后能够做
到自动打包的能力，对于打包领域基本上代码不需要做任何变动，收货领域直接调用我们提供的两个接
口就可以实现了。

这个项目虽然技术难度上不高，但是我觉得在这个项目中，是实实在在地解决了很多的痛点。
我自己独立设计、管理好自己的排期，没有出现延期的情况下以很高的质量完成了重构；
积极学习各种知识并运用到重构过程中包括：
借鉴开源算法，实现使用DAG 图的方式来对流程进行编排、并发执行；
参考gRPC的插件功能以及Gin 框架的中间件的实现，使用基于 Go 语言的闭包和函数传递机制实现了
可注册的拦截器功能；

在项目中我积极跟各个领域协调沟通，让他们提供我们需要的接口，我积极跟pm对齐，一起梳理了业务，
弥补了我们文档这块的空白；
重构后的代码对于新同学也非常友好，能够很容易理解整个业务流程，对于新需求的开发变得很容易，而
且不再像之前一样经常触雷；
重构项目很平滑稳定地上线了，并且将接口响应时间缩减了特别的多，让线下作业效率变得更加高效了；


## 2.2 库存盘点
项目主要提供支持每日定时对站点内包裹重新扫描盘点，实时了解站内包裹积压和遗失状态，及时处理遗失件，
同时生成以下数据：last mile hub 的每日入库清单 、记录盘点清单，并支持业务在盘点作业中，更新订
单的当前站点信息及状态生成 、last mile hub 的每日盘点前 & 盘点期间出库清单、推算出历史库存件
清单，丢失件清单，生成盘点报告。

主要是一个报表类、数据统计相关的需求。

Es 和 Ck 的选型：
CK列式存储:
1. 亿行数据毫秒级响应
2. 兼容 SQL 语法，维护成本低
3. 查询和插入不支持高QPS，视集群性能在几十到几百的QPS，因此不适合一线业务直接读写，例如核心作业
流程;适合统计分析、报表监控、导出查询等操作
4. 没有删除操作，使用逻辑删除;join 性能很强
5. 多主架构，同一查询可能有数据延迟、不一致 使用final关键字保持一致
6. 适合大批量低频率插入


Redis 作用：
1.分布式锁

2.缓存
缓存的是用户查询请求，报表展示的维度主要分为两类：1是按日期维度的查询展示； 2是按region（区域）维度的查询展示，
上线后发现大多数用户在一段时间内有许多重复的查询请求，因为我们存储使用了CK，它因为基建的原因，偶尔出现不稳定的情
况，以及它的查询会占用比较多的资源，所以查询偶尔会出现比较慢的情况，所以我考虑给用户的查询增加了一个缓存，另外，
我发现线上有一个很有意思的情况，就是用户习惯在连续的两次请求里分别查询按日期维度和按region 维度的数据。所以我在
给这两个查询都增加了一个预缓存的机制，比如：用户按日期维度查询的时候，很大概率他下次就会按region 维度查询，所以
我使用go 协程对接下来的请求查询提前先做，并缓存下来。
加了缓存后，原来比较多的高达十几s的查询基本就没有再出现过了。

ClickHouse的查询缓存是一种可嵌入式的缓存机制，
可以在查询语句中使用特定的语法来启用缓存。例如，
在查询语句中添加CACHE关键字，可以将查询结果缓
存到内存中。如果相同的查询被多次执行，ClickHouse
会直接从缓存中获取结果，而不必再次执行查询，从
而提高查询的性能和吞吐量。 使用查询缓存可以提高
查询的性能和响应速度，特别是在相同的查询被多次执
行时，可以避免重复计算和查询，从而节省系统资源。

------
# 2.工作遇到的问题
## 2.1 MySQL 遇到的问题
### 2.1.1 死锁的问题
性能下降：死锁会导致事务相互等待，直到超时或者MySQL选择一个事务作为牺牲者回滚，
这样会导致事务的执行时间变长，从而影响服务的性能。
UPDATE table_name SET column1 = value1, column2 = value2, ... WHERE id=xxxx;
UPDATE table_name SET column1 = value1, column2 = value2, ... WHERE task_id=xxxxx;


MySQL死锁是指两个或多个事务相互等待对方释放资源而无法继续执行的情况。当发生死锁时，MySQL会自动选择一个事务作为死锁牺牲者进行回滚，以结束死锁。
为了排查MySQL死锁，可以执行以下步骤：
查看MySQL错误日志：在MySQL错误日志中，可以查看到死锁的详细信息，包括死锁的事务ID、死锁发生的表和行号等。可以通过查看错误日志来确定死锁发生的具体原因。
执行SHOW ENGINE INNODB STATUS命令：在MySQL命令行中执行SHOW ENGINE INNODB STATUS命令，可以查看到当前正在运行的事务、锁信息和等待情况等详细信息，通过这些信息可以判断哪些事务出现了死锁。
分析死锁日志：MySQL可以将死锁信息写入到死锁日志中，通过分析死锁日志可以找出死锁的原因和发生时间等信息。可以通过设置参数innodb_print_all_deadlocks=1来开启死锁日志记录。
使用锁监控工具：MySQL提供了一些锁监控工具，如Percona Toolkit等，可以帮助我们更方便地分析死锁问题。使用这些工具可以从多个维度查看死锁信息，包括锁等待时间、锁争用次数等。
优化查询语句和事务设计：死锁问题往往与查询语句和事务设计有关。优化查询语句和事务设计可以减少死锁的发生。例如，可以使用合适的索引、分批次操作数据、尽量避免长事务等。

### 2.1.3 查询慢
场景就是一个临期商品的数据表，里面有 type（商品类型字段），在这个字段上面有建立一个索引；
数据很离散六百万的数据，按 type 查询出来大概还有80万

MySQL 只考虑过滤出来数据的百分比，而不考虑这些数据是否分布在哪些盘片上面，所以他load 盘片多了，特别是他数据很离散的情况下，他会做大量
的回表查询并且因为数据离散的原因，增加了它随机IO 的查询，还不如直接进行全表查询。

优化： 
1.大批量插入数据的时候，尽量先排序再插入

2.optimize table xxx;
> 在MySQL中，OPTIMIZE TABLE是一种用于优化表的命令。其作用是对表进行碎片整理，从而提高查询性能和减少磁盘空间的使用。
当MySQL执行UPDATE、DELETE等操作时，会释放一些空间，但并不会自动归还给操作系统。这样，表中就会存在一些未使用的空间，称为碎片。碎片会占用磁盘空间，同时也会降低查询性能。
使用OPTIMIZE TABLE命令可以将表中的碎片整理成连续的空间，从而提高查询性能和减少磁盘空间的使用。OPTIMIZE TABLE命令会执行以下操作：
重建表：OPTIMIZE TABLE会创建一个新的表，并将原表的数据复制到新表中。在复制数据的过程中，会对数据进行整理，从而消除碎片。
优化索引：OPTIMIZE TABLE会对表中的索引进行优化，从而提高查询性能。
释放空间：OPTIMIZE TABLE会释放未使用的空间，从而减少磁盘空间的使用。
需要注意的是，OPTIMIZE TABLE命令会锁定表，因此在执行期间，其他用户无法对表进行修改。因此，在执行OPTIMIZE TABLE命令之前，需要确保没有其他用户正在使用该表。
使用OPTIMIZE TABLE命令的基本语法如下：
OPTIMIZE TABLE table_name;

3. force index = null
> 在MySQL中，FORCE INDEX是一种用于强制MySQL使用指定的索引的命令。在某些情况下，MySQL的查询优化器可能会选择不是最优的索引，或者根本不使用索引。这时，可以使用FORCE INDEX命令来强制MySQL使用指定的索引。
使用FORCE INDEX命令的基本语法如下：
SELECT * FROM table_name FORCE INDEX (index_name) WHERE condition;
其中，table_name是要查询的表名，index_name是要强制使用的索引名，condition是查询条件。
需要注意的是，使用FORCE INDEX命令可能会导致查询性能下降，因为MySQL会强制使用指定的索引，而不考虑其他可能更优的索引。因此，在使用FORCE INDEX命令之前，需要仔细评估索引的选择，确认强制使用指定的索引能够提高查询性能。
另外，MySQL也提供了另外一种命令INDEX HINT，可以指定MySQL使用某个索引，但不强制必须使用，语法如下：
SELECT * FROM table_name USE INDEX (index_name) WHEREcondition;
其中，USE INDEX命令会提示MySQL使用指定的索引，但不强制必须使用，MySQL仍然会根据查询条件和其他因素来选择最优的索引。

几十 s 级别优化到 s 级别

---------
可维护性高的代码通常具有以下特点：
1. 易读性好：代码易于阅读和理解，命名清晰、简洁明了、注释充分、结构清晰、缩进合理等。
2. 易修改性好：代码易于修改和扩展，代码的耦合度低、模块化设计、接口设计良好、代码复用度高等。
3. 可测试性好：代码易于测试和调试，代码的单元测试、集成测试、自动化测试等能够覆盖到代码的各个部分，测试结果可靠。
4. 健壮性好：代码具有强健性，能够处理各种边界情况，不易出现异常和错误情况。
5. 可维护性好：代码易于维护和修复问题，能够快速定位和修复问题，代码的修改和维护不会对其他部分产生影响。
6. 性能高效：代码的执行效率高，不会因为性能问题而影响用户体验。
7. 可扩展性好：代码易于扩展和升级，能够满足未来的需求和变化，不会影响到已有的功能和代码。
维护性高的代码能够帮助开发人员更加高效、稳定地进行开发和维护工作，也更容易被其他开发人员理解和修改。同时，可维护性高的代码也能够降低软件开发和维护的成本和风险。为了提高代码的可维护性，开发人员需要注重代码的设计、命名、注释、测试等方面，同时也需要采用合适的编码规范和设计模式等技术手段来优化代码的质量和可维护性。


代码重构是指在不改变代码外部行为的情况下，通过修改代码内部结构和实现方式，以提高代码的质量、可读性、可维护性和可扩展性的过程。代码重构的目的是为了使代码更易于理解、修改和扩展，从而提高代码的质量和可维护性。

以下是代码重构的一些常见目的：
1. 提高代码质量：通过消除代码中的重复、复杂度和不必要的依赖等问题，可以提高代码的质量和可读性。
2. 简化代码实现：通过使用更简单、更清晰的实现方式，可以使代码更易于理解和修改。
3. 提高可维护性：通过将代码分解为逻辑上独立的模块、使用设计模式、引入注释和文档等最佳实践，可以提高代码的可维护性和稳定性。
4. 改进代码性能：通过优化代码结构和实现方式，可以提高代码的性能和效率。
5. 改进代码可读性：通过使用有意义的变量和函数名、遵循编码规范、引入注释和文档等最佳实践，可以提高代码的可读性，使其更易于理解和修改。
6. 改进代码扩展性：通过模块化设计、使用设计模式等最佳实践，可以提高代码的可扩展性，使其更易于扩展和适应变化的需求。
总之，代码重构的目的是为了改善代码的质量和可维护性，使其更易于理解、修改和扩展。通过不断地进行代码重构，可以使代码保持健康和稳定，从而更好地满足业务需求和用户需求。

---------
## 网络
> [计算机网络](../../notes/计算机网络3.md) 

## 操作系统
> [计算机操作系统](../../notes/计算机操作系统.md) 

## 数据库
> [MySQL](../../notes/MySQL.md) </br>
> [Redis面经](../redis/RedisQuestion.md)

## 消息队列
> [消息队列](../../notes/消息队列.md)

## Go
> [Go](../Go/go-interview.md)